# BorshchevY_platform
BorshchevY Platform repository

## ДЗ: Сетевое взаимодействие Pod, сервисы

### 1. Почему следующая конфигурация валидна, но не имеет смысла?
```yaml
livenessProbe:
  exec:
    command:
      - 'sh'
      - '-c'
      - 'ps aux | grep my_web_server_process'
```
Данная проверка не всегда имеет смысл, так как процесс может висеть в списке процессов, но по той или иной причине на самом деле не выполнять никакой полезной работы ("завис" и т.п.). В таком случае под не будет перезапущен и клиенты нашего сервиса будут получать ошибки.
Но основная проблема этой проверки, на мой взгляд, заключается в том, что в ней мы проверяем наличие нашего процесса в списке запущенных процессов, но если наше приложение по каким-то причинам "сдохнет", то контейнер, и соответственно под, будут перезапущены автоматически по причине того, что упал наш родительский родительский процесс (PID 1) и, соотвественно, до нашей проверки `livenessProbe` не дойдет дело, да и в принципе она теряет смысл. 

### 2. Бывают ли ситуации, когда она все-таки имеет смысл?
Одним из вариантов применения данной проверки могла бы быть ситуация, когда у нас в контейнере запущено два зависимых процесса - например, веб-сервер и in-memory кеш. И тогда мы можем чекать этой проверкой зависимый процесс (тот что не с PID 1 - в нашем случае, in-memory кеш) и в случае, если он упадет, `livenessProbe` проверка зафейлится и под будет перезапущен.

### Задание со * - DNS через MetalLB
Если не ошибаюсь, то с версии `v1.20alpha` добавили поддержку мультипротоколов в сервисе типа LB.
Описание [здесь](https://github.com/kubernetes/kubernetes/pull/94028/files)
```go
...
	// alpha: v1.20
	//
	// Enables the usage of different protocols in the same Service with type=LoadBalancer
	MixedProtocolLBService featuregate.Feature = "MixedProtocolLBService"
...
```
Попробовал оба варианта - и с описанием в каждого протокола в отдельном манифесте с привязкой двух сервисов к одному IP адресу и с описанием двух протоколов в одном магифесте.  
Изначально в ДЗ сдал манифест для сервиса с описанием сразу двух протоколов.  
Но потом упал на автотестах и переделал на реализацию, ожидаемую в автотестах.

### Задание со * - Ingress для Dashboard
Использовал сервис, создаваемый в рамках деплоя дашборды.
Также, добавил запись `lb.k8s.local` в файле hosts для LB, смотрящего на ingress controller.

### Заданиe со * - Canary для Ingress
Создал два манифеста с образом `nginxdemos/hello:0.3`, который разворачивает приложение app в двух разных неймспейсах - `app-prod` и `app-uat`. Добавил в файле `hosts` запись `app.lb.k8s.local`, которая указывает на LB, созданный для ingress controller-a.
В манифесте, которое будет выступать в качестве `canary` деплоймента (неймспейс `app-uat`) для ингресса добавил аннотации
```yaml
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-by-header: "app-uat"
```  
Тестировал в chrom-e - установил плагин (`ModHeader`) который позволяет добавлять произвольные поля в заголовки запросов.
Добавляя в заголовок запроса поле `app-uat` со значеним `always` получал в браузере деплоймент uat-версии приложения. Удаляя это дополнительный заголовок или выставляя его значение в `never` возвращался к продуктовой версии приложения.